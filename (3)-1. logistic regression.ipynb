{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "068bfe2c",
   "metadata": {},
   "source": [
    "* https://jovian.ai/aakashns/03-logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a0f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f774032",
   "metadata": {},
   "source": [
    "### training & test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7d6d5",
   "metadata": {},
   "source": [
    "* If we did not download the data, we should download it for the first time. \n",
    "* In downloading, we do not differentiate training and testing data.\n",
    "* root='data/' : We are going to create a folder called data, and then save the MNIST data within the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2475b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data/\n",
      "    Split: Train\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "# dataset = MNIST(root='data/', download=True) # We have to download the data if we haven't.\n",
    "dataset = MNIST(root='data/', download=False)\n",
    "print(dataset)\n",
    "print(len(dataset)) # number of image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cbf9a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data/\n",
      "    Split: Test\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', train=False) # contained inside the already-downloaded \"dataset\".\n",
    "print(test_dataset)\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90b2c0",
   "metadata": {},
   "source": [
    "* The 1st image of the training data\n",
    "* It says 28-by-28 pixel. label=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e0a37e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2217BB987C0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[0][1]) # label\n",
    "dataset[0][0] # image file of 1-28-by-28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ef6fe",
   "metadata": {},
   "source": [
    "### display an image\n",
    "* matplotlib.pyplot is needed in \"nicely\" visualizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bff0c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = dataset[0]\n",
    "plt.imshow(image,cmap='gray') # cmap='gray' : black-and-white setting.\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57e0ca",
   "metadata": {},
   "source": [
    "* ToTensor() : transform an image to a tensor object.\n",
    "* Each of our tensor object (which was originally an image) has 1-28-28 dimension. For the 1st digit, 1 means one color setting, and 3 means RGB setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f4f6300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "dataset = MNIST(root='data/', train=True, transform=transforms.ToTensor()) # Our training data now contains a tensor instead of an image.\n",
    "img_tensor, label = dataset[0] # Each element of the dataset has imag_tensor and a label.\n",
    "print(img_tensor.shape, label)\n",
    "print(img_tensor.dtype) # we may change it by adjusting the default dtype before calling the image file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aed821",
   "metadata": {},
   "source": [
    "* 0=black / 1=white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3e32b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "        [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "        [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "        [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2217cf43bb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(img_tensor[0,10:15,10:15])\n",
    "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5d0a5",
   "metadata": {},
   "source": [
    "### split the training dataset into training & validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd09fbb",
   "metadata": {},
   "source": [
    "* Before actually using the test data, we pick the best model by adjusting hyperparameters etc.\n",
    "* When splitting, we should randomly split it, because the order (indices) of the observations may have some pattern when it was first made, for instance, ordered by size.\n",
    "* For train_loader, we shuffle=True, since we want to randomly select batches for each step of SGD. On the other hand, we shuffe=False in the val_loader, because we do not need randomness here.\n",
    "* Even for shuffle=True, we get multiple batches, but it is not random.\n",
    "* len $\\neq$ tensor.numel(): len() of an array (or tensor) is the number of the biggest-bracketed subtensors.\n",
    "* If we want to analyze just a single batch, use for loop and break immediately.\n",
    "* random_split() can split the data to even three or more parts when lengths=[a1,a2,a3] has more than two digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a196c188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "torch.manual_seed(0)\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000]) # How to be splitted depends on the seed.\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8373f137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, torch.Size([1, 28, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [xb for xb, yb in train_ds]\n",
    "len(a), a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73fae8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size=128 # Each batch contains 128 image objects (tensors).\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True) # shuffle=True : ensures different batch for each epoch\n",
    "val_loader = DataLoader(val_ds, batch_size) # shuffle=False is default. DataLoader creates a random generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28283407",
   "metadata": {},
   "source": [
    "### predicting with a randomly generated model\n",
    "* First we are fitting this with a linear model: nn.Linear(number of input variables, number of output variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd4c73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "input_size = 28 * 28\n",
    "num_classes = 10\n",
    "model =  nn.Linear(input_size, num_classes) # randomly initialized a model with input = 1-by-784 matrix, not 1-28-28 tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9e75426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict([('weight', Parameter containing:\n",
       "               tensor([[ 0.0213, -0.0300, -0.0341,  ...,  0.0103,  0.0106, -0.0244],\n",
       "                       [ 0.0259,  0.0142, -0.0166,  ...,  0.0166, -0.0323,  0.0258],\n",
       "                       [ 0.0295, -0.0161,  0.0326,  ..., -0.0073,  0.0200,  0.0111],\n",
       "                       ...,\n",
       "                       [-0.0228,  0.0257, -0.0325,  ...,  0.0221, -0.0112,  0.0060],\n",
       "                       [ 0.0251,  0.0324,  0.0299,  ..., -0.0235,  0.0252,  0.0309],\n",
       "                       [ 0.0282,  0.0304, -0.0162,  ...,  0.0219, -0.0098,  0.0286]],\n",
       "                      requires_grad=True)),\n",
       "              ('bias',\n",
       "               Parameter containing:\n",
       "               tensor([ 0.0356,  0.0348,  0.0209, -0.0119, -0.0128, -0.0210, -0.0208, -0.0309,\n",
       "                       -0.0295, -0.0007], requires_grad=True))]),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'in_features': 784,\n",
       " 'out_features': 10}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight\n",
    "model.bias\n",
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e336bcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0213, -0.0300, -0.0341,  ...,  0.0103,  0.0106, -0.0244],\n",
      "        [ 0.0259,  0.0142, -0.0166,  ...,  0.0166, -0.0323,  0.0258],\n",
      "        [ 0.0295, -0.0161,  0.0326,  ..., -0.0073,  0.0200,  0.0111],\n",
      "        ...,\n",
      "        [-0.0228,  0.0257, -0.0325,  ...,  0.0221, -0.0112,  0.0060],\n",
      "        [ 0.0251,  0.0324,  0.0299,  ..., -0.0235,  0.0252,  0.0309],\n",
      "        [ 0.0282,  0.0304, -0.0162,  ...,  0.0219, -0.0098,  0.0286]],\n",
      "       requires_grad=True)\n",
      "torch.Size([10])\n",
      "Parameter containing:\n",
      "tensor([ 0.0356,  0.0348,  0.0209, -0.0119, -0.0128, -0.0210, -0.0208, -0.0309,\n",
      "        -0.0295, -0.0007], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "print(model.weight)\n",
    "print(model.bias.shape)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9072a0",
   "metadata": {},
   "source": [
    "* Before predicting by model(), we should match the size of the input. It should be 784-vector instead of 1-28-28 tensor. Otherwise it returns an error as following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c7c674c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 8, 7, 1, 7, 0, 1, 7, 7, 0, 6, 8, 3, 9, 1, 7, 2, 8, 7, 5, 9, 7, 2,\n",
      "        5, 1, 6, 0, 9, 0, 2, 2, 0, 4, 4, 7, 8, 3, 1, 6, 5, 4, 1, 6, 8, 0, 4, 0,\n",
      "        3, 2, 8, 5, 0, 1, 7, 2, 4, 3, 8, 8, 6, 6, 6, 3, 7, 4, 8, 6, 8, 3, 5, 3,\n",
      "        0, 4, 6, 6, 2, 5, 9, 6, 4, 2, 2, 9, 7, 4, 1, 0, 2, 5, 7, 5, 9, 7, 8, 2,\n",
      "        7, 6, 6, 9, 1, 6, 0, 5, 3, 7, 9, 1, 0, 8, 1, 6, 8, 5, 3, 1, 3, 1, 8, 5,\n",
      "        9, 8, 1, 7, 1, 2, 7, 1])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "#    outputs = model(images) # This gives us an error, because the input of the model should be 784, not 1-28-28.\n",
    "    outputs = model(images.reshape(-1,784)) # Now it has the correct size: 128-by-784.\n",
    "#    print(outputs)\n",
    "    print(outputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c260f",
   "metadata": {},
   "source": [
    "* __Memorize the the whole box beneath!__\n",
    "* It is widely spread to define a class that inherits nn.Module, and define a forward(self, X_input) inside the class.\n",
    "* ```def forward(self, xb)```: extends the model into a function that takes the input data (xb).\n",
    "* We have to unsqueeze a former-tensor xb into a $N\\times p$ matrix. In this case, it is $1\\times p$ matrix, since there is only a single observation.\n",
    "* As the output of ```nn.Linear(input_size, num_classes)```, we can make a prediction function that takes input matrices. xb under ```def forward``` is just that. That is why we can put xb (reshaped) as the input of self.linear().\n",
    "* Through this, we make model = MnistModel() as a function that takes a 4-way array (N-1-28-28) as input and outputs N-by-K (K=10) matrix.\n",
    "* https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3d961c",
   "metadata": {},
   "source": [
    "* Although we are defining ```forward```, we cannot change the name \"forward\" to a different name, say \"forwarda\". We may define its functionalities inside, but the name itself should be maintained.\n",
    "* Just like ```.backward()``` means computing gradients, ```forward``` means computing the output values for the given input tensors.\n",
    "* We may ask \"Exactly which functionalities do we inherit from nn.Module?\" But the conventional answer is, \"We just use it as a template to characterizing the layers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8745da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self): # First, we inherit the function nn.Linear.\n",
    "        super().__init__() # We __init__ nn.Module instead.\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, xb): # Second, we extend this to a function that takes an input xb. Now the class becomes a function of xb.\n",
    "        xb = xb.reshape(-1,784) # 1 by 784 matrix -> may serve as an input. cannot be 784-lengthed vector.\n",
    "        out = self.linear(xb) # Only after this extension, can we apply nn.Linear.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3f3183",
   "metadata": {},
   "source": [
    "* model.linear is the output of nn.Linear.\n",
    "* Thus we can pick out its weight with model.linear.weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf2554b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0136,  0.0135,  0.0049,  ...,  0.0291, -0.0221, -0.0172],\n",
       "         [-0.0273, -0.0186, -0.0005,  ..., -0.0242,  0.0342,  0.0215],\n",
       "         [ 0.0157,  0.0297, -0.0084,  ...,  0.0235,  0.0159,  0.0078],\n",
       "         ...,\n",
       "         [ 0.0316, -0.0068, -0.0335,  ...,  0.0208,  0.0104, -0.0273],\n",
       "         [ 0.0224,  0.0355, -0.0006,  ...,  0.0254, -0.0355,  0.0121],\n",
       "         [-0.0128,  0.0136, -0.0291,  ...,  0.0190,  0.0319, -0.0229]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0315,  0.0297,  0.0126, -0.0310, -0.0331,  0.0213,  0.0122,  0.0193,\n",
       "          0.0270, -0.0079], requires_grad=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MnistModel()\n",
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fee802f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "input shape : torch.Size([128, 1, 28, 28])\n",
      "outputs.shape : torch.Size([128, 10])\n",
      "sample outputs : \n",
      " tensor([[-0.3474,  0.2321, -0.1543, -0.0855, -0.1076, -0.1940, -0.1600,  0.1201,\n",
      "         -0.4006,  0.1249],\n",
      "        [-0.3997, -0.0522,  0.1505,  0.0557, -0.1361, -0.1593,  0.0436,  0.1200,\n",
      "         -0.1479,  0.2464]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break\n",
    "\n",
    "print('input shape :', images.shape)\n",
    "print('outputs.shape :', outputs.shape)\n",
    "print('sample outputs : \\n', outputs[:2].data) # print only 2 of them out of 128 pictures in one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15866437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "probs = F.softmax(outputs, dim=1) # make each as a vector (array of dim=1).\n",
    "print(\"sample probabilities:\\n\", probs[:2].data) # .data : without gradients\n",
    "print(\"sum:\", torch.sum(probs[0]).item()) # nearly 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a134bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "# print(max_probs)\n",
    "print(preds)\n",
    "labels # Prediction does not match well with the randomly generated model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89016eaa",
   "metadata": {},
   "source": [
    "### accuracy with cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[:2])\n",
    "print(torch.sum(preds==labels).item() / len(preds)) \n",
    "len(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63796f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(l1, l2):\n",
    "    return torch.sum(l1==l2).item() / len(l1) # .item() : returns the value of the tensor.\n",
    "\n",
    "accuracy(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ccb5c",
   "metadata": {},
   "source": [
    "* cross entropy: the bigger the probability of the correct state is, the small its value becomes. (Wrong states does not count since its $y_j=0$ for all $j$ that is not the correct index.)\n",
    "* Cross entropy is better than the above accuracy, since it is differentiable.\n",
    "* We will use cross entropy for loss function, but will report accuracy at the end.\n",
    "* cross_entropy(input1=$N\\times K$-matrix, input2=$N$-lengthed vector): input1 is not limited in values, but input2 must have 0,$\\cdots$,K-1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15bd18",
   "metadata": {},
   "source": [
    "* With $p$ and $q$ being the true and estimated (with softmax function) probability respectively, and $\\chi$ being the support, cross entropy is defined as $H(p,q)=E_p(\\log q(X)) = -\\sum_{x\\in \\chi}p(x)\\log q(x)$, and is estimated by $\\hat{H}(p,q) = \\frac{1}{N} \\sum_{i=1}^{N} \\log q(x_i)$ (mean version), or $\\hat{H}(p,q) =\\sum_{i=1}^{N} \\log q(x_i)$ (sum version). We follow the mean version as default.\n",
    "* Note that $q$ is estimated by softmaxing the input matrix by rows, and then $q(x_i)$ is estimated only for the realized value $x_i$.\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cross entropy with F.cross_entropy.\n",
    "obj1 = torch.rand(10).reshape(-1,2)\n",
    "obj2 = torch.tensor([0,1,1,0,0])\n",
    "F.cross_entropy(obj1, obj2)\n",
    "\n",
    "# compute manually\n",
    "logq = torch.log(torch.exp(obj1) / torch.sum(torch.exp(obj1), axis=1).reshape(-1,1))\n",
    "np.mean([-logq[i,obj2[i]].item() for i in range(5)])\n",
    "\n",
    "# verification\n",
    "F.cross_entropy(obj1, obj2)-np.mean([-logq[i,obj2[i]].item() for i in range(5)]) # verify that they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5305a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "loss_fn = F.cross_entropy\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a939f5",
   "metadata": {},
   "source": [
    "### optimizer\n",
    "* specify model and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimzer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5795ec",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70023106",
   "metadata": {},
   "source": [
    "* loss_batch() : update the parameters once, and get the result of that single iteration\n",
    "* loss_batch() takes a single batch of data as the input, while evaluate takes the whole validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645935a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None, metric=None): # take a single batch xb, yb as input.\n",
    "    # calculate loss\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb) # .item : value of the loss\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward() # compute gradients\n",
    "        opt.step() # update parameters\n",
    "        opt.zero_grad() # reset gradients\n",
    "        \n",
    "    metric_result = None\n",
    "    if metric is not None:\n",
    "        metric_result = metric(preds, yb) # compute the metric\n",
    "    \n",
    "    return loss.item(), len(xb), metric_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd261784",
   "metadata": {},
   "source": [
    "* zip(iterator1, iterator2, iterator3, ...) : pair the first elements together, second elements together, and so on.\n",
    "* zip(\\*list) where list contains multiple objects.\n",
    "* It sorts the same objects together.\n",
    "* https://stackoverflow.com/questions/29139350/difference-between-ziplist-and-ziplist/29139418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c969f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example of zip\n",
    "# a = (\"John\", \"Charles\", \"Mike\")\n",
    "# b = (\"Jenny\", \"Christy\", \"Monica\")\n",
    "# x = zip(a, b)\n",
    "# print(tuple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134cd517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (2, 5), (3, 6)]\n",
      "(1, 4)\n",
      "(2, 5)\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "p = ([1,2,3], [4,5,6])\n",
    "# p = [(1,2,3), (4,5,6)] # same result\n",
    "d = zip(*p) # * : unpacks the list. remove the outer bracket. \n",
    "print(list(d)) # -> therefore this line is equivalent to zip([1,2,3], [4,5,6]) \n",
    "a,b,c = zip(*p)\n",
    "print(a); print(b); print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fbdd7f",
   "metadata": {},
   "source": [
    "* results = [(a1,b1,c1), (a2,b2,c2), (a3,b3,c3), $\\cdots$]\n",
    "* zip(\\*results) gives us 3 objects : [a1,a2,a3. $\\cdots$], [b1,b2,b3,$\\cdots$], [c1,c2,c3,$\\cdots$]\n",
    "* np.multiply(tuple1, tuple2) : tuple1 and tuple2 should have the same size. output is also the same, but in np.array.\n",
    "* np.multiply((1,2,3), (1,2,3)  ) = array([1,4,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4431f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [loss_batch(model, loss_fn, xb, yb, metric=accuracy) for xb,yb in val_loader]\n",
    "results\n",
    "a,b,c = zip(*results)\n",
    "print(a[:10], b[:10], c[:10])\n",
    "np.multiply(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032061a1",
   "metadata": {},
   "source": [
    "* evaluate() : assess the performance of the model on validation dataset. Since it takes the whole validation dataset as the input, unlike loss_batch that only takes a single batch, it focuses on \"average performance\".\n",
    "* Although it contains loss_batch() inside, it has opt=None, so it does not update any parameter. It only measures the accuracy when applied to validation data.\n",
    "* In fact, we do not need torch.no_grad(), since we are not taking gradient descent.\n",
    "* In every epoch, we are updating the parameters 391 times (=number of batches of the training dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e17975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, valid_dl, metric = None):\n",
    "    with torch.no_grad(): # gradient descent is done only for training set, and validation set is just for reporting purpose.\n",
    "        # pass each batch through the model\n",
    "        results = [loss_batch(model, loss_fn, xb, yb, metric=metric) for xb,yb in valid_dl]\n",
    "        \n",
    "        # separate losses, counts, metrics\n",
    "        losses, nums, metrics = zip(*results) \n",
    "        # total size of the dataset\n",
    "        total = np.sum(nums) # total number of obs in the dataset = 10000\n",
    "        # average loss across batches\n",
    "        avg_loss = np.sum(np.multiply(losses, nums)) / total \n",
    "        avg_metric = None\n",
    "        if metric is not None:\n",
    "            # average of metric across batches\n",
    "            avg_metric = np.sum(np.multiply(metrics, nums)) / total # weighted sum of accuracy.\n",
    "    return avg_loss, total, avg_metric # focuses on \"average\" of all batches (or whole validation dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbe5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1) # -, : We don't care about the maximum value, but we save its label as pred.s\n",
    "    return torch.sum(preds==labels).item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc54fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, total, val_acc = evaluate(model, loss_fn, val_loader, metric =  accuracy)\n",
    "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a4e992",
   "metadata": {},
   "source": [
    "* fit() : fitting procedure. Inside it, we update it with loss_batch() and assess its peformance on validation set with evaluate().\n",
    "* For each batch of the training dataset, we update once.\n",
    "* function.\\_\\_name\\_\\_ : the name of the function\n",
    "* loss,\\_,\\_ = loss_batch(model, loss_fn, xb, yb, opt) : Only save the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric=None):\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        # training\n",
    "        for xb,yb in train_dl:\n",
    "#             loss,_,_ = loss_batch(model, loss_fn, xb, yb, opt) # update the model parameters and save the loss results\n",
    "            loss_batch(model, loss_fn, xb, yb, opt) # update the model parameters. not record the loss.\n",
    "            \n",
    "        # evaluation\n",
    "        result = evaluate(model, loss_fn, valid_dl, metric)\n",
    "        val_loss, total, val_metric =  result\n",
    "        \n",
    "        # print progress\n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, val_loss))\n",
    "        else:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'.format(epoch+1, epochs, val_loss, metric.__name__, val_metric))\n",
    "            \n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e96865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MnistModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a7f37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = fit(10, model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)\n",
    "accuracies = [result[2] for result in history] # This is how they pick out accuracies from each iteration from a non-matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6f281",
   "metadata": {},
   "source": [
    "* The accuracy increases, but stops around 0.85.\n",
    "* There can be two reasons, first of which is that 1) the learning rate is too big. However a more likely reason is that 2) the model is ot good. Nothing says that they should have a linear relationship. We will therefore apply a model that can characterize a nonlinear relatioship next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45054240",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs No. of epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43c3ac",
   "metadata": {},
   "source": [
    "## Application to the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5f80a",
   "metadata": {},
   "source": [
    "### loading and displaying the test data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba79916",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root='data/', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf13fc",
   "metadata": {},
   "source": [
    "* torch.unsqueeze(k): add another dimnesion in the k-th position. For a 1-28-28 tensor object tensor becomes 1-1-28-28 tensor, because it added one more dimension in the 0-th posision of the new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "print(x.shape)\n",
    "print(torch.unsqueeze(x, 0).shape, torch.unsqueeze(x, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69605480",
   "metadata": {},
   "source": [
    "* torch.unsqueeze(img, 0) converts a single 1-28-28 tensor into a 1-1-28-28 tensor, which is virtually the same with a batch of a single observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee72bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "print(torch.unsqueeze(img, 0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a4a52",
   "metadata": {},
   "source": [
    "### prediction on each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df35f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model): # the model here is the model that we already fitted with the training & validation datasets.\n",
    "    xb=torch.unsqueeze(img, 0)\n",
    "    yb=model(xb)\n",
    "    _, preds = torch.max(yb, dim=1) # only take the index of the maximizer.\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_dataset[10]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65aea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_dataset[193]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = test_dataset[1839]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b20381",
   "metadata": {},
   "source": [
    "### prediction accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "test_loss, total, test_acc = evaluate(model, loss_fn, test_loader, metric=accuracy)\n",
    "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c8815",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "* It would be a good idea to save the model that we already fitted, so that we would not need to refit it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'data/mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35a765",
   "metadata": {},
   "source": [
    "* We cannot bring a model out of nowhere. We first need to form its class, which in our case is MnistModel()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=MnistModel()\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_state_dict(torch.load('data/mnist-logistic.pth'))\n",
    "print(model2.state_dict()) \n",
    "torch.sum(list(model.parameters())[0]!=list(model2.parameters())[0]) # model1 = model2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2, _, acc2 = evaluate(model2, loss_fn, test_loader, metric=accuracy)\n",
    "print(round(loss2, 4), acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f50790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ff6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22fe03d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503fbd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
