{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7924e98",
   "metadata": {},
   "source": [
    "# <PART 1: Individual steps>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88dfff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e484bd3",
   "metadata": {},
   "source": [
    "## Step1: Call the training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f059f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data/\n",
      "    Split: Train\n",
      "60000\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data/\n",
      "    Split: Test\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST(root='data/', download=False, train=True)\n",
    "print(dataset)\n",
    "print(len(dataset))\n",
    "test_dataset = MNIST(root='data/', train=False)\n",
    "print(test_dataset)\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226aff3",
   "metadata": {},
   "source": [
    "## Step2: Call each image file as a picture or a tensor object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34b1c0",
   "metadata": {},
   "source": [
    "### picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd382f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af5d4b",
   "metadata": {},
   "source": [
    "### tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade1d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "dataset = MNIST(root = 'data/', train=True, transform = transforms.ToTensor())\n",
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)\n",
    "print(img_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb659305",
   "metadata": {},
   "source": [
    "## Step3: Split the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d116d7",
   "metadata": {},
   "source": [
    "* random_split() splits the 60000 samples in the training dataset into training (500000) and validation (100000) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29154ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a,b=random_split(torch.tensor([1,2,3]), [2,1])\n",
    "# a[0],a[1], b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d0b373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "torch.manual_seed(0)\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d61a8",
   "metadata": {},
   "source": [
    "* DataLoader() splits the 50000 images in the training dataset into 391 batches, each with 128 objects (and one batch with the remaining). \n",
    "* train_loader and val_loader generates an object that contains 391 and 79 batches of data. Each batch has len=2, with the elements being a 128-1-28-28 tensor object and a 128 labels. However the last batch may have a size fewer than 128.\n",
    "* Each batch contains two types of data: 128 image files and 128 labels, named as xb and yb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f94e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size=128\n",
    "train_loader=DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader=DataLoader(val_ds, batch_size)\n",
    "print(len(train_loader)) # 391 batches in total.\n",
    "print(len(val_loader)) # 79 batches in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84594eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128  80]\n",
      "[128  16]\n"
     ]
    }
   ],
   "source": [
    "length_vec = []\n",
    "for xb, yb in train_loader:\n",
    "    length_vec.append(len(xb))\n",
    "print(np.array(length_vec)[[0,len(train_loader)-1]]) # all batches with 128 image files, last batch 80 image files\n",
    "\n",
    "length_vec = []\n",
    "for xb, yb in val_loader:\n",
    "    length_vec.append(len(xb))\n",
    "print(np.array(length_vec)[[0,len(val_loader)-1]]) # all batches with 128 image files, last batch 16 image files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af295b4",
   "metadata": {},
   "source": [
    "* This is how we pick out an individual element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0552f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " torch.Size([128, 1, 28, 28]),\n",
       " 128,\n",
       " tensor([0, 5, 9, 7, 2, 6, 1, 6, 9, 5, 9, 9, 6, 7, 4, 5, 2, 4, 3, 3, 0, 7, 7, 7,\n",
       "         7, 8, 5, 4, 7, 5, 5, 2, 0, 4, 2, 1, 2, 2, 0, 6, 2, 9, 6, 5, 2, 3, 7, 5,\n",
       "         0, 3, 2, 3, 1, 4, 9, 5, 7, 3, 2, 5, 8, 3, 7, 7, 2, 3, 0, 2, 7, 1, 9, 4,\n",
       "         2, 6, 3, 1, 3, 6, 2, 0, 3, 8, 8, 8, 3, 4, 4, 2, 1, 5, 5, 7, 6, 7, 6, 0,\n",
       "         9, 3, 6, 1, 0, 1, 2, 1, 0, 9, 2, 3, 5, 4, 2, 2, 3, 3, 9, 3, 5, 4, 0, 9,\n",
       "         1, 8, 7, 8, 2, 5, 7, 7]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for element in train_loader:\n",
    "    break\n",
    "len(element), element[0].shape, len(element[0]), element[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749744b3",
   "metadata": {},
   "source": [
    "* This is how we pick one element from train_loader.\n",
    "* Every time we realize train_loader, we shuffle it, so we get to observe a randomly selected batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f2efcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "tensor([7, 7, 8, 0, 7, 2, 6, 0, 8, 6, 2, 7, 3, 8, 0, 2, 5, 8, 2, 1, 4, 1, 1, 7,\n",
      "        6, 1, 9, 2, 6, 2, 5, 1, 9, 4, 4, 5, 9, 7, 4, 6, 6, 7, 1, 3, 2, 1, 0, 6,\n",
      "        6, 9, 5, 0, 3, 9, 8, 1, 0, 2, 5, 1, 1, 7, 6, 4, 1, 0, 6, 1, 0, 5, 0, 9,\n",
      "        9, 4, 6, 5, 8, 1, 1, 3, 3, 3, 2, 3, 8, 7, 0, 9, 5, 2, 8, 0, 6, 1, 6, 1,\n",
      "        7, 4, 3, 0, 9, 7, 3, 6, 0, 1, 8, 1, 3, 6, 9, 2, 7, 8, 6, 3, 6, 5, 6, 7,\n",
      "        3, 9, 8, 2, 4, 0, 9, 3])\n"
     ]
    }
   ],
   "source": [
    "for dataset in train_loader: # Each dataset contains two types of data\n",
    "    print(dataset[0].shape) # 4-dim tensor: 128 tensor objects of 1-28-28.\n",
    "    print(dataset[1]) # 128 labels.\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a555e",
   "metadata": {},
   "source": [
    "## Step4: Generate a model and predict with it. (Single Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd097dfa",
   "metadata": {},
   "source": [
    "* We have 10 output variables, which eventually become the proability of each class.\n",
    "* For each output variable, we do a (generalized) linear regression: 784 inputs (weights) and 1 constant (bias).\n",
    "* nn.Linear(input variable #, output variable #)\n",
    "* We just randomly generated a model, so its accuracy is terrible at the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f7d0ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n",
      "Parameter containing:\n",
      "tensor([ 0.0356,  0.0136, -0.0266,  0.0188,  0.0147, -0.0263,  0.0197,  0.0134,\n",
      "         0.0211, -0.0227], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "input_size = 28 * 28\n",
    "num_classes = 10\n",
    "model = nn.Linear(input_size, num_classes) # randomly generate a linear model with \n",
    "print(model.weight.shape) # 784 weights allocated to each pixel for each class = 784 input variables\n",
    "print(model.bias) # 1 bias allocated to each class = 1 constant term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16de9b",
   "metadata": {},
   "source": [
    "* Recall that model inputs and outputs $N\\times p$ and $N\\times K$ matrices.\n",
    "* So we need to transform the input tensor of size=128-1-28-28 into ?-784 (=128-784) matrix, in order to use it as the input variable.\n",
    "* When transforming it, we get to line them all up in order. Since the 128 tensors is the very outer bracket, each 1-28-28 tensor is lined up as a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeca85b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 3, 4, 0, 6, 3, 8, 1, 3, 0, 2, 3, 0, 0, 2, 1, 3, 5, 5, 0, 2, 1, 8, 9,\n",
      "        4, 6, 1, 1, 0, 2, 6, 3, 7, 3, 7, 1, 9, 8, 2, 7, 6, 3, 1, 7, 2, 8, 2, 7,\n",
      "        0, 5, 7, 7, 0, 3, 8, 5, 0, 0, 3, 7, 9, 0, 3, 3, 8, 4, 7, 8, 0, 7, 0, 1,\n",
      "        2, 6, 0, 2, 6, 3, 4, 3, 7, 2, 8, 2, 0, 7, 1, 7, 6, 0, 5, 7, 0, 1, 7, 3,\n",
      "        1, 6, 5, 1, 4, 3, 7, 4, 5, 3, 6, 3, 1, 3, 3, 8, 1, 2, 0, 9, 6, 4, 8, 0,\n",
      "        7, 6, 1, 2, 1, 8, 3, 9])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "tensor([[-0.0963, -0.0282,  0.2746,  ...,  0.0257, -0.0732,  0.0228],\n",
      "        [-0.0031,  0.0441,  0.0913,  ...,  0.0692, -0.2793,  0.0609],\n",
      "        [-0.1320, -0.0080,  0.0170,  ...,  0.1333,  0.0119,  0.0865],\n",
      "        ...,\n",
      "        [-0.0188,  0.2417,  0.2082,  ...,  0.1113, -0.0292,  0.2793],\n",
      "        [-0.0085,  0.2744,  0.0149,  ...,  0.2142, -0.1590,  0.2255],\n",
      "        [-0.2558,  0.2315, -0.1850,  ...,  0.1679,  0.0668,  0.3075]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader: # just one batch : break\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    outputs = model(images.reshape(-1,784))\n",
    "    print(outputs)\n",
    "    print(outputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b18ae",
   "metadata": {},
   "source": [
    "* softmax - 1) torch.nn.functional.softmax 2) manual computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87eac6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0874, 0.0936, 0.1267,  ..., 0.0988, 0.0895, 0.0985],\n",
       "        [0.1025, 0.1075, 0.1126,  ..., 0.1102, 0.0778, 0.1093],\n",
       "        [0.0853, 0.0965, 0.0990,  ..., 0.1112, 0.0985, 0.1061],\n",
       "        ...,\n",
       "        [0.0877, 0.1138, 0.1100,  ..., 0.0999, 0.0868, 0.1181],\n",
       "        [0.0948, 0.1258, 0.0970,  ..., 0.1184, 0.0815, 0.1198],\n",
       "        [0.0703, 0.1145, 0.0755,  ..., 0.1074, 0.0971, 0.1235]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch package\n",
    "import torch.nn.functional as F\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d2ad6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.9802e-08, grad_fn=<MaxBackward1>),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual computation\n",
    "exp_outputs = torch.exp(outputs)\n",
    "manual_computed = exp_outputs / torch.sum(exp_outputs,axis=1).reshape(-1,1)\n",
    "torch.max(torch.abs(manual_computed - probs)),torch.sum(manual_computed, axis=1)  \n",
    "# We get the same results as softmax (except numerical errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ca506d",
   "metadata": {},
   "source": [
    "* torch.max : single maximum value\n",
    "* torch.max(, dim=1) : row-wise maximum $\\rightarrow$ prints maximum values & indices\n",
    "* torch.max(, dim=2) : col-wise maximum  $\\rightarrow$ prints maximum values & indices\n",
    "* tensor.item() : Given a tensor object __with a single value__, this returns the numeric value, not the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09178ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.109375"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, preds = torch.max(probs,dim=1) # two values: max values themselves & indices that have the maximum values\n",
    "torch.sum(preds==labels).item() / len(preds) # Given a tensor object with a single value, .item() gives its value, not the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058409b",
   "metadata": {},
   "source": [
    "* torch.nn.functional.cross_entropy(obj1, obj2) : $N\\times K$ matrix with prob values $\\in[0,1]$, $N$-lengthed vector with $K$ possible labels, which are $0 \\cdots (K-1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "943a58ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10]) torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3273677825927734"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs.shape, labels.shape)\n",
    "F.cross_entropy(outputs, labels).item() # just pick out the numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051621f",
   "metadata": {},
   "source": [
    "## Step5: Optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a0a8b0",
   "metadata": {},
   "source": [
    "* torch.optim.SGD(obj1, obj2) : obj1 = the parameters that we will update // obj2 = learning rate\n",
    "* When we do SGD, we take the following steps.\n",
    "* 1) predict = model(xb)\n",
    "* 2) loss = torch.nn.functional.~~ (pred, yb)\n",
    "* 3) loss.backward: loss is a loss function defined in torch.nn.functional (e.g.) torch.nn.functional.mse_loss, torch.nn.functional.cross_entropy\n",
    "* 4) opt.step() : take a gradient descent.\n",
    "* 5) opt.zero_grad() : reset the gradient as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d718b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149380a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27aa5f1e",
   "metadata": {},
   "source": [
    "# <PART 2: Whole Algorithm>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee124e5",
   "metadata": {},
   "source": [
    "* %reset removes all objects and packages. However there should not follow anyting including comments in the same line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32da25b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae92d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4a3f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='data/', download=False, train=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f495467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x255e5a02950>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e4c9f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_ds, val_ds = random_split(train_dataset, [50000, 10000]) # split the training data into training data & validation data\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size=128\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92b1ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "input_size = 28 * 28\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c514d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cd71658",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1,input_size)\n",
    "        out = self.linear(xb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4107f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a single batch in the training dataset, we update the parameter once.\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None, metric=None): # xb: Xmat // yb: label \n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb) # 1st step\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward() # 2nd step\n",
    "        opt.step() # 3rd step\n",
    "        opt.zero_grad() # 4th step\n",
    "    \n",
    "    metric_result = None\n",
    "    if metric is not None:\n",
    "        metric_result = metric(preds, yb) # accuracy\n",
    "    \n",
    "    return loss.item(), len(xb), metric_result # xb : 128-1-28-28 tensor: len(xb)=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "883c7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
    "    results = [loss_batch(model, loss_fn, xb, yb, metric = metric) for xb,yb in valid_dl]\n",
    "    losses, nums, metrics = zip(*results)\n",
    "    total = sum(nums)\n",
    "    avg_loss = np.sum(np.multiply(losses, nums)) / total # average loss\n",
    "    avg_metric = None\n",
    "    if metric is not None:\n",
    "            avg_metric = np.sum(np.multiply(metrics, nums)) / total # average accuracy\n",
    "    return avg_loss, total, avg_metric # We later use this for assessing the performance when applied to validation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0d8730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1) # Since softmax is monotonely increasing function, there is no need to F.softmax().\n",
    "    return torch.sum(preds==labels).item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "066bce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric=None):\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl: # update the parameters with the training data.\n",
    "            loss_batch(model, loss_fn, xb, yb, opt) # We do not have to save the results as _,_,_. \"opt\" (the procedure of updating) is what matters.\n",
    "        result = evaluate(model, loss_fn, valid_dl, metric)\n",
    "        val_loss, total, val_metric = result\n",
    "        \n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, val_loss))\n",
    "        else:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'.format(epoch+1, epochs, val_loss, metric.__name__, val_metric)  )\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30f04347",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "model = MnistModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ce46c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.9569, accuracy: 0.6429\n",
      "Epoch [2/20], Loss: 1.6873, accuracy: 0.7222\n",
      "Epoch [3/20], Loss: 1.4865, accuracy: 0.7491\n",
      "Epoch [4/20], Loss: 1.3351, accuracy: 0.7715\n",
      "Epoch [5/20], Loss: 1.2185, accuracy: 0.7866\n",
      "Epoch [6/20], Loss: 1.1268, accuracy: 0.8000\n",
      "Epoch [7/20], Loss: 1.0532, accuracy: 0.8090\n",
      "Epoch [8/20], Loss: 0.9929, accuracy: 0.8169\n",
      "Epoch [9/20], Loss: 0.9426, accuracy: 0.8224\n",
      "Epoch [10/20], Loss: 0.9001, accuracy: 0.8279\n",
      "Epoch [11/20], Loss: 0.8637, accuracy: 0.8316\n",
      "Epoch [12/20], Loss: 0.8321, accuracy: 0.8333\n",
      "Epoch [13/20], Loss: 0.8045, accuracy: 0.8354\n",
      "Epoch [14/20], Loss: 0.7799, accuracy: 0.8386\n",
      "Epoch [15/20], Loss: 0.7582, accuracy: 0.8405\n",
      "Epoch [16/20], Loss: 0.7386, accuracy: 0.8427\n",
      "Epoch [17/20], Loss: 0.7210, accuracy: 0.8443\n",
      "Epoch [18/20], Loss: 0.7049, accuracy: 0.8461\n",
      "Epoch [19/20], Loss: 0.6904, accuracy: 0.8483\n",
      "Epoch [20/20], Loss: 0.6769, accuracy: 0.8502\n"
     ]
    }
   ],
   "source": [
    "num_iter=20\n",
    "history = fit(num_iter, model, loss_fn, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32c80dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [history[i][2] for i in range(num_iter) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f68a894c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy vs No, of epochs')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(accuracies, '-*') # - : line   //   * : point figure\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs No, of epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82d297f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6477, Accuracy: 0.8629\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', download=False, train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "test_loss, total, test_acc = evaluate(model, loss_fn, test_loader, metric=accuracy)\n",
    "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2115211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'data/mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a350d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5455ef33",
   "metadata": {},
   "source": [
    "## <Part 3: Call the model and apply>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45abfdcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69dd601",
   "metadata": {},
   "source": [
    "* We should call all the necessary packages and the functions. \n",
    "* We do not have to call the training and validation dataset. In fact this is the whole point of saving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ec3e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3dfcd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=28*28; num_classes=10\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1,input_size)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "# Given a single batch in the training dataset, we update the parameter once.\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None, metric=None): # xb: Xmat // yb: label \n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb) # 1st step\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward() # 2nd step\n",
    "        opt.step() # 3rd step\n",
    "        opt.zero_grad() # 4th step\n",
    "    \n",
    "    metric_result = None\n",
    "    if metric is not None:\n",
    "        metric_result = metric(preds, yb) # accuracy\n",
    "    \n",
    "    return loss.item(), len(xb), metric_result # xb : 128-1-28-28 tensor: len(xb)=128\n",
    "\n",
    "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
    "    results = [loss_batch(model, loss_fn, xb, yb, metric = metric) for xb,yb in valid_dl]\n",
    "    losses, nums, metrics = zip(*results)\n",
    "    total = sum(nums)\n",
    "    avg_loss = np.sum(np.multiply(losses, nums)) / total # average loss\n",
    "    avg_metric = None\n",
    "    if metric is not None:\n",
    "            avg_metric = np.sum(np.multiply(metrics, nums)) / total # average accuracy\n",
    "    return avg_loss, total, avg_metric # We later use this for assessing the performance when applied to validation dataset.\n",
    "\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1) # Since softmax is monotonely increasing function, there is no need to F.softmax().\n",
    "    return torch.sum(preds==labels).item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8539350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_call = MnistModel()\n",
    "model_call.load_state_dict(torch.load('data/mnist-logistic.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b26d291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6477, Accuracy: 0.8629\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', download=False, train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "test_loss, total, test_acc = evaluate(model_call, loss_fn, test_loader, metric=accuracy)\n",
    "print('Loss: {:.4f}, Accuracy: {:.4f}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444bfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b10239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302ca0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
